{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Tracking and Behavior Recognition using Deep Learning\n",
    "\n",
    "<hr />\n",
    "\n",
    "#### Implemented in Python, pytorch, using HDMB51 dataset\n",
    "\n",
    "Ensure that pytorch, torchvision, opencv and cuda are installed to execute this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version -- 2.0.0\n",
      "PyAV version -- 10.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#check for pytorch version\n",
    "print(f\"PyTorch version -- {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "from torchvision import get_video_backend\n",
    "from torchvision.models.video import r3d_18 \n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import av\n",
    "import random\n",
    "print(f\"PyAV version -- {av.__version__}\")\n",
    "\n",
    "SEED = 491\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyav\n"
     ]
    }
   ],
   "source": [
    "# diagnostics for PyAV installation.\n",
    "def run_av_diagnostics():\n",
    "    import av\n",
    "    av.open(\"video_data/brush_hair/brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ba_goo_4.avi\")\n",
    "    print(get_video_backend())\n",
    "    av.logging.set_level(av.logging.ERROR)\n",
    "    if not hasattr(av.video.frame.VideoFrame, 'pict_type'):\n",
    "      print(\"Unavailable!\")\n",
    "\n",
    "run_av_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m       out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog_softmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(out), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m       \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m---> 14\u001b[0m check \u001b[39m=\u001b[39m Model()\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     15\u001b[0m out \u001b[39m=\u001b[39m check(torch\u001b[39m.\u001b[39mrandn(\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m , \u001b[39m8\u001b[39m, \u001b[39m112\u001b[39m,\u001b[39m112\u001b[39m)\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m     16\u001b[0m out\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[0;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[0;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# set up a sample pytorch class to check if it works\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(Model, self).__init__()\n",
    "      self.base_model = nn.Sequential(*list(r3d_18(pretrained=False).children())[:-1])\n",
    "      self.fc1 = nn.Linear(512, 51)\n",
    "\n",
    "  def forward(self, x):\n",
    "      out = self.base_model(x).squeeze(4).squeeze(3).squeeze(2)\n",
    "      print(\"size after pretrained model \", out.size())\n",
    "      out = torch.log_softmax(self.fc1(out), dim=1)\n",
    "      return out\n",
    "\n",
    "check = Model().cuda()\n",
    "out = check(torch.randn(16, 3 , 8, 112,112).cuda())\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is pytorch working\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def update_acc(self, val, n=1):\n",
    "        self.val = val/n\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model using PyTorch\n",
    "class VideoRecognitionModel(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(VideoRecognitionModel, self).__init__()\n",
    "      self.base_model = nn.Sequential(*list(r3d_18(pretrained=True).children())[:-1])\n",
    "      self.fc1 = nn.Linear(512, 51)\n",
    "      self.fc2 = nn.Linear(51, 51) \n",
    "      self.dropout = nn.Dropout2d(0.3) \n",
    "\n",
    "  def forward(self, x):\n",
    "      out = self.base_model(x).squeeze(4).squeeze(3).squeeze(2)\n",
    "      out = F.relu(self.fc1(out)) \n",
    "      out = self.dropout(out) \n",
    "      out = torch.log_softmax(self.fc2(out), dim=1)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    config = {}\n",
    "    config['log_interval'] = 100\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    flag = 0\n",
    "    Loss, Acc = AverageMeter(), AverageMeter()\n",
    "    start = time.time()\n",
    "    for batch_id, data in enumerate(loader):\n",
    "        data, target = data[0], data[-1]\n",
    "        # print(\"here\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "           data = data.cuda()\n",
    "           target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        Loss.update(loss.item(), data.size(0))\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "        correct += num_corrects\n",
    "\n",
    "        Acc.update_acc(num_corrects, data.size(0))\n",
    "\n",
    "        if flag!= 0 and batch_id%config['log_interval'] == 0:\n",
    "           print('Train Epoch: {} Batch [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {}/{} ({:.0f})%'.format(\n",
    "                epoch, batch_id * len(data), len(loader.dataset),\n",
    "                100. * batch_id / len(loader), Loss.avg, correct, Acc.count, 100. * Acc.avg))\n",
    "        flag = 1\n",
    "\n",
    "    #total_loss /= len(loader.dataset) \n",
    "    print('Train Epoch: {} Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f})%'.format(\n",
    "         epoch, Loss.avg, correct, Acc.count, 100. * Acc.avg ))\n",
    "    print(f\"Takes {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model, loader, text='Validation'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    Loss, Acc = AverageMeter(), AverageMeter()\n",
    "    with torch.no_grad():\n",
    "         for batch_id, data in enumerate(loader):\n",
    "             data, target = data[0], data[-1]\n",
    "\n",
    "             if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "             output = model(data)\n",
    "             loss = F.nll_loss(output, target)\n",
    "             total_loss += loss.item()\n",
    "\n",
    "             Loss.update(loss.item(), data.size(0))\n",
    "\n",
    "             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "             num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "             correct += num_corrects\n",
    "\n",
    "             Acc.update_acc(num_corrects, data.size(0))\n",
    "           \n",
    "    total_loss /= len(loader.dataset)\n",
    "    print(text + ' Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f})%'.format(\n",
    "         Loss.avg, correct, Acc.count , 100. * Acc.avg ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [15:25<00:00,  2.19s/it]  \n",
      "100%|██████████| 423/423 [18:36<00:00,  2.64s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples 7754\n",
      "number of validation samples 388\n",
      "number of test samples 3234\n"
     ]
    }
   ],
   "source": [
    "# Datasets and Dataloaders for model training ..\n",
    "\n",
    "val_split = 0.05\n",
    "num_frames = 16 # 16\n",
    "clip_steps = 50\n",
    "num_workers = 8\n",
    "pin_memory = True\n",
    "train_tfms = torchvision.transforms.Compose([\n",
    "                                 T.ToPILImage(),\n",
    "                                 T.Resize((128, 171)),\n",
    "                                 T.RandomCrop((112,112)),                                 \n",
    "                                 T.RandomHorizontalFlip(),\n",
    "                                 T.ToTensor(),\n",
    "                                 T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "                                 lambda x: np.rollaxis(x.numpy(), 0, 3)\n",
    "                               ])  \n",
    "test_tfms =  torchvision.transforms.Compose([\n",
    "                                             T.ToPILImage(),\n",
    "                                             T.Resize((128, 171)),\n",
    "                                             T.CenterCrop((112,112)),\n",
    "                                             T.ToTensor(),                                             \n",
    "                                             T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "                                             ])\n",
    "hmdb51_train = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
    "                                                step_between_clips = clip_steps, fold=1, train=True,\n",
    "                                                transform=train_tfms, num_workers=num_workers)\n",
    "\n",
    "\n",
    "hmdb51_test = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
    "                                                step_between_clips = clip_steps, fold=1, train=False,\n",
    "                                                transform=test_tfms, num_workers=num_workers)\n",
    "      \n",
    "total_train_samples = len(hmdb51_train)\n",
    "total_val_samples = round(val_split * total_train_samples)\n",
    "\n",
    "print(f\"number of train samples {total_train_samples}\")\n",
    "print(f\"number of validation samples {total_val_samples}\")\n",
    "print(f\"number of test samples {len(hmdb51_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Recognition Model Training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAction Recognition Model Training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, total_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     train(config, model, train_loader, optimizer, epoch)\n\u001b[0;32m     38\u001b[0m     test(config, model, val_loader, text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[42], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, model, loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m Loss, Acc \u001b[39m=\u001b[39m AverageMeter(), AverageMeter()\n\u001b[0;32m      9\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m     11\u001b[0m     data, target \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m], data[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[39m# print(\"here\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\hmdb51.py:149\u001b[0m, in \u001b[0;36mHMDB51.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    146\u001b[0m _, class_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[sample_index]\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(video)\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m video, audio, class_index\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    226\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:266\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndimension()\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    269\u001b[0m         \u001b[39m# if 2D image, add channel dimension (CHW)\u001b[39;00m\n\u001b[0;32m    270\u001b[0m         pic \u001b[39m=\u001b[39m pic\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 4 dimensions."
     ]
    }
   ],
   "source": [
    "bs = 4\n",
    "lr = 1e-2\n",
    "gamma = 0.7\n",
    "total_epochs = 10\n",
    "config = {}\n",
    "num_workers = 0\n",
    "\n",
    "kwargs = {'num_workers':num_workers, 'pin_memory':True} if torch.cuda.is_available() else {'num_workers':num_workers}\n",
    "#kwargs = {'num_workers':num_workers}\n",
    "#kwargs = {}\n",
    "\n",
    "hmdb51_train_v1, hmdb51_val_v1 = random_split(hmdb51_train, [total_train_samples - total_val_samples,\n",
    "                                                                       total_val_samples])\n",
    "\n",
    "#hmdb51_train_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n",
    "#hmdb51_val_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n",
    "#hmdb51_test.video_clips.compute_clips(16, 1, frame_rate=30)\n",
    "\n",
    "#train_sampler = RandomClipSampler(hmdb51_train_v1.video_clips, 5)\n",
    "#test_sampler = UniformClipSampler(hmdb51_test.video_clips, 5)\n",
    "  \n",
    "train_loader = DataLoader(hmdb51_train_v1, batch_size=bs, shuffle=True, **kwargs)\n",
    "val_loader   = DataLoader(hmdb51_val_v1, batch_size=bs, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(hmdb51_test, batch_size=bs, shuffle=False, **kwargs)\n",
    "\n",
    "model = VideoRecognitionModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "print(\"Action Recognition Model Training\")\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    train(config, model, train_loader, optimizer, epoch)\n",
    "    test(config, model, val_loader, text=\"Validation\")\n",
    "    scheduler.step()\n",
    "\n",
    "test(config, model, test_loader, text=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
